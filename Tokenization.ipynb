{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementing Tokenization",
   "id": "5fc26cbda96ad8d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26.0"
   ],
   "id": "2bc30bcd9cade422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T22:22:23.924889Z",
     "start_time": "2025-06-25T22:22:23.889631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "6be64614a2b21d21",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prodesk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/prodesk/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is a tokenizer and why do we use it?\n",
    "\n",
    "Tokenizers play a pivotal role in natural language processing, segmenting text into smaller units known as tokens. These tokens are subsequently transformed into numerical representations called token indices, which are directly employed by deep learning algorithms.\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%201.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>"
   ],
   "id": "63baea3d14d679bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Types of tokenizer\n",
    "\n",
    "The meaningful representation can vary depending on the model in use. Various models employ distinct tokenization algorithms, and you will broadly cover the following approaches. Transforming text into numerical values might appear straightforward initially, but it encompasses several considerations that must be kept in mind.\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%202.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>\n"
   ],
   "id": "edb2acfba5128faf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T22:23:40.075734Z",
     "start_time": "2025-06-25T22:23:39.958421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"This is a sample sentence for tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ],
   "id": "62b8b3d6e552d774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T22:49:47.108661Z",
     "start_time": "2025-06-25T22:49:47.102178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This showcases word_tokenize from NLTK library\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ],
   "id": "f200e6d2cc2de8eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T22:51:45.063632Z",
     "start_time": "2025-06-25T22:51:44.460997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "# Making a list of the tokens and printing the list\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)\n",
    "\n",
    "# Showing token details\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}, Dependency: {token.dep_}\")"
   ],
   "id": "860b7d66f7abca88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
      "Token: I, POS: PRON, Lemma: I, Dependency: nsubj\n",
      "Token: could, POS: AUX, Lemma: could, Dependency: aux\n",
      "Token: n't, POS: PART, Lemma: not, Dependency: neg\n",
      "Token: help, POS: VERB, Lemma: help, Dependency: ROOT\n",
      "Token: the, POS: DET, Lemma: the, Dependency: det\n",
      "Token: dog, POS: NOUN, Lemma: dog, Dependency: dobj\n",
      "Token: ., POS: PUNCT, Lemma: ., Dependency: punct\n",
      "Token: Ca, POS: AUX, Lemma: can, Dependency: aux\n",
      "Token: n't, POS: PART, Lemma: not, Dependency: neg\n",
      "Token: you, POS: PRON, Lemma: you, Dependency: nsubj\n",
      "Token: do, POS: VERB, Lemma: do, Dependency: ROOT\n",
      "Token: it, POS: PRON, Lemma: it, Dependency: dobj\n",
      "Token: ?, POS: PUNCT, Lemma: ?, Dependency: punct\n",
      "Token: Do, POS: AUX, Lemma: do, Dependency: aux\n",
      "Token: n't, POS: PART, Lemma: not, Dependency: neg\n",
      "Token: be, POS: AUX, Lemma: be, Dependency: ROOT\n",
      "Token: afraid, POS: ADJ, Lemma: afraid, Dependency: acomp\n",
      "Token: if, POS: SCONJ, Lemma: if, Dependency: mark\n",
      "Token: you, POS: PRON, Lemma: you, Dependency: nsubj\n",
      "Token: are, POS: AUX, Lemma: be, Dependency: advcl\n",
      "Token: ., POS: PUNCT, Lemma: ., Dependency: punct\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:12:05.300726Z",
     "start_time": "2025-06-26T09:12:05.267219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Unicorns are real. I sae a unicorn yesterday.\"\n",
    "token = word_tokenize(text)\n",
    "print(\"Tokens:\", token)"
   ],
   "id": "b40509b4831a9f2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Unicorns', 'are', 'real', '.', 'I', 'sae', 'a', 'unicorn', 'yesterday', '.']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Subword-based tokenizer",
   "id": "74ec9443a16aa65f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:13.612513Z",
     "start_time": "2025-06-26T09:13:11.016362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ],
   "id": "fc723ecb3d57e49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unigram and SentencePiece Tokenizer",
   "id": "55ca22cf7586cad8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:14:56.740095Z",
     "start_time": "2025-06-26T09:14:54.083838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ],
   "id": "d3abd32fabd24743",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['笆！BM', '笆》aught', '笆［e', '笆》oken', 'ization', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization with PyTorch",
   "id": "b8d63f0be346e697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:15:37.216205Z",
     "start_time": "2025-06-26T09:15:37.204205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
   ],
   "id": "e71cc16cb9bb70d9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:15:51.940496Z",
     "start_time": "2025-06-26T09:15:51.919296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ],
   "id": "1ce8d77398c482b7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:16:00.583488Z",
     "start_time": "2025-06-26T09:16:00.575596Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(dataset[0][1])",
   "id": "4ce7cd288fa6ec74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Token indices",
   "id": "ba8175fac764dd2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:20:51.964593Z",
     "start_time": "2025-06-26T09:20:51.960867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, t in data_iter:\n",
    "        yield tokenizer(t)"
   ],
   "id": "7863de62bba195a7",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:22:18.785966Z",
     "start_time": "2025-06-26T09:22:18.781791Z"
    }
   },
   "cell_type": "code",
   "source": "my_iter = yield_tokens(dataset)",
   "id": "4d6f21421154c572",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:20:55.351293Z",
     "start_time": "2025-06-26T09:20:55.346912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = [next(my_iter), next(my_iter)]\n",
    "print(\"First two tokenized sentences:\", results)"
   ],
   "id": "ca84371a57a507bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two tokenized sentences: [['introduction', 'to', 'nlp'], ['basics', 'of', 'pytorch']]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Out-of-vocabulary (OOV) tokens",
   "id": "d5a275be9f41991f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:20:57.291093Z",
     "start_time": "2025-06-26T09:20:57.197447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = build_vocab_from_iterator(my_iter, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "id": "326deb163a45df93",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:21:49.305382Z",
     "start_time": "2025-06-26T09:21:49.296435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence_ = next(iterator) # Get the first tokenized sentence\n",
    "    token_indices_ = [vocab[token] for token in tokenized_sentence_] # Get token indices\n",
    "    return tokenized_sentence_, token_indices_"
   ],
   "id": "1d3b41cccd5b7fb9",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:22:20.108513Z",
     "start_time": "2025-06-26T09:22:20.104516Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iter)",
   "id": "52ba35727ddbe6d7",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:22:34.672244Z",
     "start_time": "2025-06-26T09:22:34.667995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ],
   "id": "b6edb8e9d2e46641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['introduction', 'to', 'nlp']\n",
      "Token Indices: [0, 0, 1]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:27:06.339417Z",
     "start_time": "2025-06-26T09:27:05.567582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lines = [\"IBM taught me tokenization\",\n",
    "         \"Special tokenizers are ready and they will blow your mind\",\n",
    "         \"just saying hi!\"]\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Convert tokens to indices\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_itos())"
   ],
   "id": "b7eaa17bdc7fa883",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines after adding special tokens:\n",
      " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
      "Token IDs for 'tokenization': ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:30:41.369134Z",
     "start_time": "2025-06-26T09:30:41.361941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nex_line = \"I learned about embeddings and attention mechanisms.\"\n",
    "\n",
    "# Tokenize the new line\n",
    "tokenize_new_line = tokenizer_en(nex_line)\n",
    "tokenize_new_line = ['<bos>'] + tokenize_new_line + ['<eos>']\n",
    "\n",
    "# Pad the new line to match the max length\n",
    "new_line_padded = tokenize_new_line + ['<pad>'] * (max_length - len(tokenize_new_line))\n",
    "\n",
    "# Convert the new line to indices\n",
    "new_line_indices = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
    "\n",
    "print(\"New line after adding special tokens and padding:\", new_line_indices)"
   ],
   "id": "83cad649f5cd08ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New line after adding special tokens and padding: [2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b75a2cf08179adef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
